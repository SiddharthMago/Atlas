{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az5UbQ7QI3iM",
        "outputId": "8b42ee03-4a3f-4e7a-e847-c4cc88d1c228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnH4Lt0kI6bz",
        "outputId": "7379a12c-f938-4324-d4e6-8b40734fcb90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_scatter-2.1.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt25cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_sparse-0.6.18%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt25cu124\n",
            "Looking in links: https://data.pyg.org/whl/nightly/torch-2.5.1+cu124.html\n",
            "Collecting pyg-lib\n",
            "  Downloading https://data.pyg.org/whl/nightly/torch-2.5.0%2Bcu124/pyg_lib-0.4.0.dev20250210%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyg-lib\n",
            "Successfully installed pyg-lib-0.4.0.dev20250210+pt25cu124\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-a60xfcg8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-a60xfcg8\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit bb6601c8666e69205a7a4d1d981b771e9bae6880\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.11.11)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2025.1.31)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.7.0-py3-none-any.whl size=1173586 sha256=46b2faeb610a2f65e659a9888c2b0e1e429d97d1e9e95cc4e2c7a9d6ed5a2200\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pgnwg_vi/wheels/93/bb/85/bfec4ee59b2563f74ec87cc2c91c6a4d3e40d3dcdec8ee5afe\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n"
          ]
        }
      ],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "znY7yC7UKlE6"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "def create_atlas_graph(data):\n",
        "    \"\"\"\n",
        "    Creates a directed graph for the input.\n",
        "\n",
        "    Parameters:\n",
        "        data (list): A list of place names (countries/cities).\n",
        "\n",
        "    Returns:\n",
        "        G (networkx.DiGraph): A directed graph.\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    for place in data:\n",
        "        G.add_node(place)\n",
        "        last_letter = place[-1].lower()  # last letter of the name, case-insensitive comparison\n",
        "        for candidate in data:\n",
        "            if candidate[0].lower() == last_letter:\n",
        "                G.add_edge(place, candidate)\n",
        "\n",
        "    return G\n",
        "\n",
        "# Read the list of countries from file.\n",
        "with open(\"/content/countries.txt\", \"r\") as file:\n",
        "    countries = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "country_graph = create_atlas_graph(countries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-xTa4EmJtfm",
        "outputId": "37bda974-adb3-4d05-e61a-6c0d7c0ba67f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyG Data object will have 196 nodes and 2055 edges.\n",
            "Node2Vec Epoch 10, Loss: 2.4903\n",
            "Node2Vec Epoch 20, Loss: 1.9478\n",
            "Node2Vec Epoch 30, Loss: 1.6251\n",
            "Node2Vec Epoch 40, Loss: 1.4046\n",
            "Node2Vec Epoch 50, Loss: 1.2585\n",
            "Node2Vec Epoch 60, Loss: 1.1743\n",
            "Node2Vec Epoch 70, Loss: 1.1143\n",
            "Node2Vec Epoch 80, Loss: 1.0877\n",
            "Node2Vec Epoch 90, Loss: 1.0680\n",
            "Node2Vec Epoch 100, Loss: 1.0395\n",
            "Node2Vec Epoch 110, Loss: 1.0313\n",
            "Node2Vec Epoch 120, Loss: 1.0196\n",
            "Node2Vec Epoch 130, Loss: 1.0165\n",
            "Node2Vec Epoch 140, Loss: 1.0007\n",
            "Node2Vec Epoch 150, Loss: 1.0024\n",
            "Node2Vec Epoch 160, Loss: 0.9972\n",
            "Node2Vec Epoch 170, Loss: 0.9911\n",
            "Node2Vec Epoch 180, Loss: 0.9971\n",
            "Node2Vec Epoch 190, Loss: 0.9865\n",
            "Node2Vec Epoch 200, Loss: 0.9848\n",
            "Node2Vec Epoch 210, Loss: 0.9838\n",
            "Node2Vec Epoch 220, Loss: 0.9826\n",
            "Node2Vec Epoch 230, Loss: 0.9731\n",
            "Node2Vec Epoch 240, Loss: 0.9755\n",
            "Node2Vec Epoch 250, Loss: 0.9783\n",
            "Node2Vec Epoch 260, Loss: 0.9775\n",
            "Node2Vec Epoch 270, Loss: 0.9691\n",
            "Node2Vec Epoch 280, Loss: 0.9774\n",
            "Node2Vec Epoch 290, Loss: 0.9740\n",
            "Node2Vec Epoch 300, Loss: 0.9744\n",
            "Node2Vec Epoch 310, Loss: 0.9677\n",
            "Node2Vec Epoch 320, Loss: 0.9704\n",
            "Node2Vec Epoch 330, Loss: 0.9726\n",
            "Node2Vec Epoch 340, Loss: 0.9734\n",
            "Node2Vec Epoch 350, Loss: 0.9624\n",
            "Node2Vec Epoch 360, Loss: 0.9678\n",
            "Node2Vec Epoch 370, Loss: 0.9716\n",
            "Node2Vec Epoch 380, Loss: 0.9649\n",
            "Node2Vec Epoch 390, Loss: 0.9626\n",
            "Node2Vec Epoch 400, Loss: 0.9722\n",
            "Node2Vec Epoch 410, Loss: 0.9678\n",
            "Node2Vec Epoch 420, Loss: 0.9650\n",
            "Node2Vec Epoch 430, Loss: 0.9631\n",
            "Node2Vec Epoch 440, Loss: 0.9695\n",
            "Node2Vec Epoch 450, Loss: 0.9638\n",
            "Node2Vec Epoch 460, Loss: 0.9626\n",
            "Node2Vec Epoch 470, Loss: 0.9638\n",
            "Node2Vec Epoch 480, Loss: 0.9718\n",
            "Node2Vec Epoch 490, Loss: 0.9663\n",
            "Node2Vec Epoch 500, Loss: 0.9638\n",
            "Node2Vec Epoch 510, Loss: 0.9792\n",
            "Node2Vec Epoch 520, Loss: 0.9586\n",
            "Node2Vec Epoch 530, Loss: 0.9677\n",
            "Node2Vec Epoch 540, Loss: 0.9671\n",
            "Node2Vec Epoch 550, Loss: 0.9611\n",
            "Node2Vec Epoch 560, Loss: 0.9676\n",
            "Node2Vec Epoch 570, Loss: 0.9587\n",
            "Node2Vec Epoch 580, Loss: 0.9661\n",
            "Node2Vec Epoch 590, Loss: 0.9648\n",
            "Node2Vec Epoch 600, Loss: 0.9661\n",
            "Node2Vec Epoch 610, Loss: 0.9684\n",
            "Node2Vec Epoch 620, Loss: 0.9660\n",
            "Node2Vec Epoch 630, Loss: 0.9620\n",
            "Node2Vec Epoch 640, Loss: 0.9612\n",
            "Node2Vec Epoch 650, Loss: 0.9636\n",
            "Node2Vec Epoch 660, Loss: 0.9695\n",
            "Node2Vec Epoch 670, Loss: 0.9652\n",
            "Node2Vec Epoch 680, Loss: 0.9589\n",
            "Node2Vec Epoch 690, Loss: 0.9618\n",
            "Node2Vec Epoch 700, Loss: 0.9623\n",
            "Node2Vec Epoch 710, Loss: 0.9596\n",
            "Node2Vec Epoch 720, Loss: 0.9655\n",
            "Node2Vec Epoch 730, Loss: 0.9648\n",
            "Node2Vec Epoch 740, Loss: 0.9600\n",
            "Node2Vec Epoch 750, Loss: 0.9699\n",
            "Node2Vec Epoch 760, Loss: 0.9655\n",
            "Node2Vec Epoch 770, Loss: 0.9617\n",
            "Node2Vec Epoch 780, Loss: 0.9683\n",
            "Node2Vec Epoch 790, Loss: 0.9557\n",
            "Node2Vec Epoch 800, Loss: 0.9712\n",
            "Node2Vec Epoch 810, Loss: 0.9651\n",
            "Node2Vec Epoch 820, Loss: 0.9619\n",
            "Node2Vec Epoch 830, Loss: 0.9653\n",
            "Node2Vec Epoch 840, Loss: 0.9584\n",
            "Node2Vec Epoch 850, Loss: 0.9581\n",
            "Node2Vec Epoch 860, Loss: 0.9577\n",
            "Node2Vec Epoch 870, Loss: 0.9558\n",
            "Node2Vec Epoch 880, Loss: 0.9608\n",
            "Node2Vec Epoch 890, Loss: 0.9596\n",
            "Node2Vec Epoch 900, Loss: 0.9583\n",
            "Node2Vec Epoch 910, Loss: 0.9631\n",
            "Node2Vec Epoch 920, Loss: 0.9671\n",
            "Node2Vec Epoch 930, Loss: 0.9593\n",
            "Node2Vec Epoch 940, Loss: 0.9588\n",
            "Node2Vec Epoch 950, Loss: 0.9618\n",
            "Node2Vec Epoch 960, Loss: 0.9643\n",
            "Node2Vec Epoch 970, Loss: 0.9598\n",
            "Node2Vec Epoch 980, Loss: 0.9680\n",
            "Node2Vec Epoch 990, Loss: 0.9598\n",
            "Node2Vec Epoch 1000, Loss: 0.9654\n",
            "Final Node2Vec training loss: 1.0240036518871785\n",
            "Node2Vec link prediction AUC: 0.7803942671426287\n",
            "GAE: Training edges: 1644 Test edges: 411\n",
            "GAE Epoch 20, Loss: 1.3076\n",
            "GAE Epoch 40, Loss: 1.2728\n",
            "GAE Epoch 60, Loss: 1.2587\n",
            "GAE Epoch 80, Loss: 1.2367\n",
            "GAE Epoch 100, Loss: 1.2532\n",
            "GAE Epoch 120, Loss: 1.2543\n",
            "GAE Epoch 140, Loss: 1.2342\n",
            "GAE Epoch 160, Loss: 1.2460\n",
            "GAE Epoch 180, Loss: 1.2509\n",
            "GAE Epoch 200, Loss: 1.2584\n",
            "GAE Epoch 220, Loss: 1.2446\n",
            "GAE Epoch 240, Loss: 1.2442\n",
            "GAE Epoch 260, Loss: 1.2503\n",
            "GAE Epoch 280, Loss: 1.2382\n",
            "GAE Epoch 300, Loss: 1.2346\n",
            "GAE Epoch 320, Loss: 1.2394\n",
            "GAE Epoch 340, Loss: 1.2303\n",
            "GAE Epoch 360, Loss: 1.2455\n",
            "GAE Epoch 380, Loss: 1.2467\n",
            "GAE Epoch 400, Loss: 1.2340\n",
            "GAE Epoch 420, Loss: 1.2223\n",
            "GAE Epoch 440, Loss: 1.2261\n",
            "GAE Epoch 460, Loss: 1.2283\n",
            "GAE Epoch 480, Loss: 1.2356\n",
            "GAE Epoch 500, Loss: 1.2139\n",
            "GAE Epoch 520, Loss: 1.2498\n",
            "GAE Epoch 540, Loss: 1.2328\n",
            "GAE Epoch 560, Loss: 1.2268\n",
            "GAE Epoch 580, Loss: 1.2465\n",
            "GAE Epoch 600, Loss: 1.2270\n",
            "GAE Epoch 620, Loss: 1.2121\n",
            "GAE Epoch 640, Loss: 1.2493\n",
            "GAE Epoch 660, Loss: 1.2229\n",
            "GAE Epoch 680, Loss: 1.2280\n",
            "GAE Epoch 700, Loss: 1.2329\n",
            "GAE Epoch 720, Loss: 1.2311\n",
            "GAE Epoch 740, Loss: 1.2245\n",
            "GAE Epoch 760, Loss: 1.2290\n",
            "GAE Epoch 780, Loss: 1.2354\n",
            "GAE Epoch 800, Loss: 1.2275\n",
            "GAE Epoch 820, Loss: 1.2137\n",
            "GAE Epoch 840, Loss: 1.2202\n",
            "GAE Epoch 860, Loss: 1.2433\n",
            "GAE Epoch 880, Loss: 1.2369\n",
            "GAE Epoch 900, Loss: 1.2106\n",
            "GAE Epoch 920, Loss: 1.2235\n",
            "GAE Epoch 940, Loss: 1.2333\n",
            "GAE Epoch 960, Loss: 1.2251\n",
            "GAE Epoch 980, Loss: 1.2268\n",
            "GAE Epoch 1000, Loss: 1.2260\n",
            "GAE link prediction AUC: 0.7431876439282268\n",
            "\n",
            "-----------------------------------------------------\n",
            "Node2Vec link prediction AUC: 0.7803942671426287\n",
            "embedding_dim:  13\n",
            "walk_length:  10\n",
            "context_size:  5\n",
            "walks_per_node:  10\n",
            "num_negative_samples:  1\n",
            "nv_epochs:  1000\n",
            "learning_rate:  0.008\n",
            "\n",
            "-----------------------------------------------------\n",
            "GAE link prediction AUC: 0.7431876439282268\n",
            "hidden_dim:  17\n",
            "split_ratio:  0.2\n",
            "gae_epochs:  1000\n",
            "learning_rate:  0.005\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import Node2Vec, GCNConv, GAE\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 1. Convert the NetworkX Graph to a PyG Data Object with Node Features\n",
        "# Build a mapping from country name to numeric index.\n",
        "mapping = {country: i for i, country in enumerate(countries)}\n",
        "\n",
        "# Create node features.\n",
        "# Here we use a simple 2D feature: [normalized(first_letter), normalized(last_letter)]\n",
        "def get_feature(country):\n",
        "    country = country.lower()\n",
        "    first = (ord(country[0]) - ord('a')) / 25.0  # Normalize a=0,...,z=1\n",
        "    last  = (ord(country[-1]) - ord('a')) / 25.0\n",
        "    return [first, last]\n",
        "\n",
        "# Build feature matrix (num_nodes x 2)\n",
        "num_nodes = len(countries)\n",
        "x = torch.tensor([get_feature(country) for country in countries], dtype=torch.float)\n",
        "\n",
        "# Create the edge list (in numeric indices) from the NetworkX graph.\n",
        "edge_list = []\n",
        "for source, target in country_graph.edges():\n",
        "    # Use the mapping to get numeric indices.\n",
        "    i = mapping[source]\n",
        "    j = mapping[target]\n",
        "    edge_list.append([i, j])\n",
        "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "\n",
        "print(\"PyG Data object will have {} nodes and {} edges.\".format(num_nodes, edge_index.size(1)))\n",
        "\n",
        "# Create PyG Data object.\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "# 3. Unsupervised Link Prediction with Node2Vec (PyG Implementation)\n",
        "# Define Node2Vec parameters.\n",
        "embedding_dim = 13\n",
        "walk_length = 10\n",
        "context_size = 5\n",
        "walks_per_node = 10\n",
        "num_negative_samples = 1\n",
        "\n",
        "nv_epochs = 1000\n",
        "learning_rate_nv = 0.008\n",
        "batch_size_nv = 128\n",
        "\n",
        "# Create Node2Vec model (using the graph’s edge_index).\n",
        "node2vec = Node2Vec(\n",
        "    data.edge_index,\n",
        "    embedding_dim=embedding_dim,\n",
        "    walk_length=walk_length,\n",
        "    context_size=context_size,\n",
        "    walks_per_node=walks_per_node,\n",
        "    num_negative_samples=num_negative_samples,\n",
        "    p=1, q=1,\n",
        "    sparse=True\n",
        ")\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "node2vec = node2vec.to(device)\n",
        "optimizer_n2v = torch.optim.SparseAdam(list(node2vec.parameters()), lr=learning_rate_nv)\n",
        "\n",
        "def train_node2vec():\n",
        "    node2vec.train()\n",
        "    total_loss = 0\n",
        "    loader = node2vec.loader(batch_size=batch_size_nv, shuffle=True, num_workers=0)\n",
        "    num_epochs = nv_epochs\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        epoch_loss = 0\n",
        "        for pos_rw, neg_rw in loader:\n",
        "            pos_rw = pos_rw.to(device)\n",
        "            neg_rw = neg_rw.to(device)\n",
        "            optimizer_n2v.zero_grad()\n",
        "            loss = node2vec.loss(pos_rw, neg_rw)\n",
        "            loss.backward()\n",
        "            optimizer_n2v.step()\n",
        "            epoch_loss += loss.item()\n",
        "        avg_loss = epoch_loss / len(loader)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Node2Vec Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
        "        total_loss += avg_loss\n",
        "    return total_loss / num_epochs\n",
        "\n",
        "loss_n2v = train_node2vec()\n",
        "print(\"Final Node2Vec training loss:\", loss_n2v)\n",
        "\n",
        "# Extract the embeddings.\n",
        "embeddings = node2vec.embedding.weight.detach().cpu()\n",
        "\n",
        "# Define a link prediction evaluation function.\n",
        "def evaluate_link_prediction(embeds, pos_edges, num_negatives=None):\n",
        "    num_pos = pos_edges.size(1)\n",
        "    if num_negatives is None:\n",
        "        num_negatives = num_pos\n",
        "    neg_edges = []\n",
        "    while len(neg_edges) < num_negatives:\n",
        "        i = random.randint(0, num_nodes - 1)\n",
        "        j = random.randint(0, num_nodes - 1)\n",
        "        if [i, j] not in edge_list:\n",
        "            neg_edges.append([i, j])\n",
        "    neg_edges = torch.tensor(neg_edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    pos_scores = (embeds[pos_edges[0]] * embeds[pos_edges[1]]).sum(dim=1).numpy()\n",
        "    neg_scores = (embeds[neg_edges[0]] * embeds[neg_edges[1]]).sum(dim=1).numpy()\n",
        "\n",
        "    scores = np.concatenate([pos_scores, neg_scores])\n",
        "    labels = np.concatenate([np.ones_like(pos_scores), np.zeros_like(neg_scores)])\n",
        "    auc = roc_auc_score(labels, scores)\n",
        "    return auc\n",
        "\n",
        "# Evaluate on all observed edges.\n",
        "pos_edges = data.edge_index\n",
        "auc_n2v = evaluate_link_prediction(embeddings, pos_edges)\n",
        "print(\"Node2Vec link prediction AUC:\", auc_n2v)\n",
        "\n",
        "# 4. Unsupervised Link Prediction with a GNN (Graph Autoencoder using GCN)\n",
        "# Define a simple two-layer GCN encoder.\n",
        "class GCNEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
        "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "hidden_dim = 17\n",
        "learning_rate_gnn = 0.005\n",
        "split_ratio = 0.2\n",
        "gae_epochs = 1000\n",
        "\n",
        "encoder = GCNEncoder(in_channels=data.num_node_features, out_channels=hidden_dim)\n",
        "model = GAE(encoder).to(device)\n",
        "data = data.to(device)\n",
        "optimizer_gae = torch.optim.Adam(model.parameters(), lr=learning_rate_gnn)\n",
        "\n",
        "# Function to mask (remove) a portion of edges for testing.\n",
        "def mask_edges(data, test_ratio=split_ratio):\n",
        "    # Convert edge_index to list format.\n",
        "    edges = data.edge_index.cpu().numpy().T.tolist()\n",
        "    num_edges = len(edges)\n",
        "    num_test = int(test_ratio * num_edges)\n",
        "    random.shuffle(edges)\n",
        "    test_edges = edges[:num_test]\n",
        "    train_edges = edges[num_test:]\n",
        "    train_edge_index = torch.tensor(train_edges, dtype=torch.long).t().contiguous().to(device)\n",
        "    test_edge_index = torch.tensor(test_edges, dtype=torch.long).t().contiguous().to(device)\n",
        "    return train_edge_index, test_edge_index\n",
        "\n",
        "train_edge_index, test_edge_index = mask_edges(data, test_ratio=0.2)\n",
        "print(\"GAE: Training edges:\", train_edge_index.size(1), \"Test edges:\", test_edge_index.size(1))\n",
        "data.train_edge_index = train_edge_index\n",
        "\n",
        "# Train the GAE (unsupervised reconstruction of the graph)\n",
        "def train_gae():\n",
        "    model.train()\n",
        "    optimizer_gae.zero_grad()\n",
        "    z = model.encode(data.x, data.train_edge_index)\n",
        "    loss = model.recon_loss(z, data.train_edge_index)\n",
        "    loss.backward()\n",
        "    optimizer_gae.step()\n",
        "    return loss.item()\n",
        "\n",
        "num_gae_epochs = gae_epochs\n",
        "for epoch in range(1, num_gae_epochs + 1):\n",
        "    loss = train_gae()\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"GAE Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate GAE link prediction performance.\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z = model.encode(data.x, data.train_edge_index)\n",
        "\n",
        "def evaluate_gae(z, pos_edge_index, num_negatives=None):\n",
        "    num_pos = pos_edge_index.size(1)\n",
        "    if num_negatives is None:\n",
        "        num_negatives = num_pos\n",
        "    neg_edges = []\n",
        "    while len(neg_edges) < num_negatives:\n",
        "        i = random.randint(0, num_nodes - 1)\n",
        "        j = random.randint(0, num_nodes - 1)\n",
        "        if [i, j] not in edge_list:\n",
        "            neg_edges.append([i, j])\n",
        "    neg_edge_index = torch.tensor(neg_edges, dtype=torch.long).t().contiguous().to(device)\n",
        "\n",
        "    pos_scores = (z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=1).cpu().numpy()\n",
        "    neg_scores = (z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=1).cpu().numpy()\n",
        "\n",
        "    scores = np.concatenate([pos_scores, neg_scores])\n",
        "    labels = np.concatenate([np.ones_like(pos_scores), np.zeros_like(neg_scores)])\n",
        "    auc = roc_auc_score(labels, scores)\n",
        "    return auc\n",
        "\n",
        "auc_gae = evaluate_gae(z, test_edge_index)\n",
        "print(\"GAE link prediction AUC:\", auc_gae)\n",
        "\n",
        "print()\n",
        "print(\"-----------------------------------------------------\")\n",
        "print(\"Node2Vec link prediction AUC:\", auc_n2v)\n",
        "print(\"embedding_dim: \", embedding_dim)\n",
        "print(\"walk_length: \", walk_length)\n",
        "print(\"context_size: \", context_size)\n",
        "print(\"walks_per_node: \", walks_per_node)\n",
        "print(\"num_negative_samples: \", num_negative_samples)\n",
        "print(\"nv_epochs: \", nv_epochs)\n",
        "print(\"learning_rate: \", learning_rate_nv)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"-----------------------------------------------------\")\n",
        "print(\"GAE link prediction AUC:\", auc_gae)\n",
        "print(\"hidden_dim: \", hidden_dim)\n",
        "print(\"split_ratio: \", split_ratio)\n",
        "print(\"gae_epochs: \", gae_epochs)\n",
        "print(\"learning_rate: \", learning_rate_gnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46Dlg9xysLSv"
      },
      "source": [
        "# Analysis & Intuition\n",
        "1. Node Features\n",
        "\n",
        "    I built a graph where each country is a node, and there's an edge from one country to another if the last letter of the first country's name matches the first letter of the second. To help represent each country, I created a very simple feature for each node: a two-number vector. The first number represents the normalized value of the country's first letter, and the second represents the normalized value of its last letter. This choice makes sense because our rule for connecting countries is based solely on these letters.\n",
        "\n",
        "2. Node2Vec Approach\n",
        "\n",
        "    For the Node2Vec model, I use a method that is a bit like teaching the computer to explore the graph. Here's how it works:\n",
        "\n",
        "    - Random Walks: The model takes random walks through the graph. This gives the model lots of examples of which countries tend to be connected or appear near each other.\n",
        "\n",
        "    - Learning by Context: While taking these walks, the model learns to predict which countries (nodes) appear together. This is called the skip-gram objective.\n",
        "\n",
        "    - Negative Sampling: To help the model understand what “unrelated” looks like, for every pair of countries that do appear together (a positive example), the model is also shown a negative example—a pair that rarely or never appears together. In my setup, for every positive pair, one negative pair is sampled.\n",
        "\n",
        "    - Link Prediction: After training, each country has a vector (its embedding). The model then uses the dot product (a measure of similarity) between two country embeddings to decide if an edge should exist. If two countries have similar embeddings, they're more likely to be connected.\n",
        "\n",
        "3. GNN (Graph Autoencoder) Approach\n",
        "\n",
        "    For the GNN approach, I use a Graph Autoencoder (GAE) built with a simple two-layer Graph Convolutional Network (GCN):\n",
        "\n",
        "    - Graph Convolution:\n",
        "    The GCN learns new representations (or embeddings) for each country by “mixing” information from its neighbors. This way, each country's new features reflect both its own simple features (first and last letters) and the structure of the graph.\n",
        "\n",
        "    - Masking Edges:\n",
        "    Before training, I remove (mask) a portion of the edges from the graph. The idea is to force the model to learn enough about the overall graph structure so it can predict these missing connections.\n",
        "\n",
        "    - Unsupervised Reconstruction: The GAE is trained to reconstruct (or predict) the presence of the edges that were masked out. This means the model adjusts its internal representations so that, when it tries to recreate the graph, it comes as close as possible to the original structure.\n",
        "\n",
        "    - Link Prediction: Once the model is trained, I evaluate it by comparing the model’s predicted scores (from the dot product of the embeddings) against the actual edges (the ones that were masked for testing). This tells me how well the model learned the underlying connectivity.\n",
        "\n",
        "4. Unsupervised Learning\n",
        "\n",
        "    Both approaches are unsupervised:\n",
        "    - Node2Vec doesn't need any labels because it learns from the way nodes appear together in random walks.\n",
        "\n",
        "    - GAE doesn't need labeled data either because it learns by trying to reconstruct the graph from which some edges have been removed.\n",
        "\n",
        "    In both cases, the training objectives are all about capturing the structure of the graph—learning what makes two nodes likely to be connected—without needing a separate “correct answer” for each edge."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
